{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94047bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cdeb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '04', '05', '06', '08', '09', '11', '12', '13', '14', '15', '16', '18', '19', '20', '21', '23', '24', '26', '28', '29', '30', '31', '33', '35', '37', '38', '41', '42', '43', '44', '45', '50', '51', '53', '54', '56', '59', '60', '62', '63', '65', '69', '70', '72', '74', '75', '76', '77', '79', '81', '82', '84', '85', '87', '88', '89', '90', '91', '94', '95', '97', '98']\n",
      "Dataset CustomImageFolder\n",
      "    Number of datapoints: 64000\n",
      "    Root location: double_mnist/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=1)\n",
      "               Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "['03', '07', '10', '22', '27', '34', '39', '40', '48', '52', '58', '61', '64', '71', '93', '99']\n",
      "tensor([23, 41,  9, 79, 82, 19, 62, 24, 50,  1, 72, 65, 70, 81, 33, 77, 15, 24,\n",
      "        75,  5, 43, 35, 19, 16, 15,  9, 59, 41, 29, 79, 91, 87, 81, 24, 38, 44,\n",
      "        16,  6, 38, 81, 14, 14, 84, 30, 33, 89, 19, 15, 13, 14, 11, 84, 69,  9,\n",
      "        16, 33, 81, 30, 37, 82,  9, 12, 62, 18])\n",
      "tensor(3)\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if not already\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data_path = 'double_mnist/train'\n",
    "val_data_path = 'double_mnist/val'\n",
    "test_data_path = 'double_mnist/test'\n",
    "class CustomImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(CustomImageFolder, self).__init__(root, transform=transform)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # Assign labels based on folder name\n",
    "        folder_name = os.path.basename(os.path.dirname(path))\n",
    "        return sample, int(folder_name)  # Convert the folder name to an integer label\n",
    "# Create ImageFolder datasets for training, validation, and test sets\n",
    "train_dataset = CustomImageFolder(train_data_path, transform=transform)\n",
    "val_dataset = CustomImageFolder(val_data_path, transform=transform)\n",
    "test_dataset = CustomImageFolder(test_data_path, transform=transform)\n",
    "print(train_dataset.classes)\n",
    "class_to_label = {str(i).zfill(2): i for i in range(100)}\n",
    "# Define batch sizes\n",
    "batch_size = 64\n",
    "print(train_dataset)\n",
    "# Create DataLoader instances for training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 10\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "class_names = val_dataset.classes\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    break\n",
    "for images, labels in val_loader:\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    print(labels[0])\n",
    "    print(labels)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2471806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_size=128, dropout_rate=0.5):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the model\n",
    "input_size = 28 * 28  # Replace with the actual input size\n",
    "num_classes = 10  # Replace with the actual number of classes\n",
    "hidden_size = 128\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = SimpleMLP(input_size, num_classes, hidden_size, dropout_rate)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model1 = SimpleMLP(input_size, num_classes, hidden_size, dropout_rate)\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3909ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Train Loss: 1.8908 - Train Accuracy: 32.95%\n",
      "Epoch 1/25 - Validation Loss: 1.8996 - Validation Accuracy: 35.45%\n",
      "Epoch 1/25 - Testing Loss: 1.9650 - Testing Accuracy: 23.30%\n",
      "Epoch 2/25 - Train Loss: 1.7160 - Train Accuracy: 40.19%\n",
      "Epoch 2/25 - Validation Loss: 1.7667 - Validation Accuracy: 40.88%\n",
      "Epoch 2/25 - Testing Loss: 1.7973 - Testing Accuracy: 31.55%\n",
      "Epoch 3/25 - Train Loss: 1.5408 - Train Accuracy: 48.11%\n",
      "Epoch 3/25 - Validation Loss: 1.5268 - Validation Accuracy: 50.45%\n",
      "Epoch 3/25 - Testing Loss: 1.5913 - Testing Accuracy: 39.88%\n",
      "Epoch 4/25 - Train Loss: 1.3430 - Train Accuracy: 55.78%\n",
      "Epoch 4/25 - Validation Loss: 1.3794 - Validation Accuracy: 54.42%\n",
      "Epoch 4/25 - Testing Loss: 1.3984 - Testing Accuracy: 49.34%\n",
      "Epoch 5/25 - Train Loss: 1.1846 - Train Accuracy: 60.82%\n",
      "Epoch 5/25 - Validation Loss: 1.1264 - Validation Accuracy: 62.33%\n",
      "Epoch 5/25 - Testing Loss: 1.1948 - Testing Accuracy: 54.24%\n",
      "Epoch 6/25 - Train Loss: 1.0752 - Train Accuracy: 63.99%\n",
      "Epoch 6/25 - Validation Loss: 1.1135 - Validation Accuracy: 62.55%\n",
      "Epoch 6/25 - Testing Loss: 1.1295 - Testing Accuracy: 57.34%\n",
      "Epoch 7/25 - Train Loss: 1.0067 - Train Accuracy: 66.01%\n",
      "Epoch 7/25 - Validation Loss: 1.0984 - Validation Accuracy: 63.09%\n",
      "Epoch 7/25 - Testing Loss: 1.1589 - Testing Accuracy: 57.86%\n",
      "Epoch 8/25 - Train Loss: 0.9559 - Train Accuracy: 67.73%\n",
      "Epoch 8/25 - Validation Loss: 0.9564 - Validation Accuracy: 67.58%\n",
      "Epoch 8/25 - Testing Loss: 1.0484 - Testing Accuracy: 61.16%\n",
      "Epoch 9/25 - Train Loss: 0.9272 - Train Accuracy: 68.64%\n",
      "Epoch 9/25 - Validation Loss: 0.9318 - Validation Accuracy: 66.21%\n",
      "Epoch 9/25 - Testing Loss: 1.0057 - Testing Accuracy: 60.70%\n",
      "Epoch 10/25 - Train Loss: 0.8996 - Train Accuracy: 69.67%\n",
      "Epoch 10/25 - Validation Loss: 0.9264 - Validation Accuracy: 68.49%\n",
      "Epoch 10/25 - Testing Loss: 0.9910 - Testing Accuracy: 63.88%\n",
      "Epoch 11/25 - Train Loss: 0.8765 - Train Accuracy: 70.52%\n",
      "Epoch 11/25 - Validation Loss: 0.9291 - Validation Accuracy: 67.69%\n",
      "Epoch 11/25 - Testing Loss: 1.0027 - Testing Accuracy: 61.66%\n",
      "Epoch 12/25 - Train Loss: 0.8579 - Train Accuracy: 71.25%\n",
      "Epoch 12/25 - Validation Loss: 0.8796 - Validation Accuracy: 70.32%\n",
      "Epoch 12/25 - Testing Loss: 0.8803 - Testing Accuracy: 67.53%\n",
      "Epoch 13/25 - Train Loss: 0.8401 - Train Accuracy: 71.84%\n",
      "Epoch 13/25 - Validation Loss: 0.9002 - Validation Accuracy: 70.03%\n",
      "Epoch 13/25 - Testing Loss: 0.9015 - Testing Accuracy: 68.06%\n",
      "Epoch 14/25 - Train Loss: 0.8258 - Train Accuracy: 72.56%\n",
      "Epoch 14/25 - Validation Loss: 0.8799 - Validation Accuracy: 70.28%\n",
      "Epoch 14/25 - Testing Loss: 0.8841 - Testing Accuracy: 67.12%\n",
      "Epoch 15/25 - Train Loss: 0.8194 - Train Accuracy: 72.91%\n",
      "Epoch 15/25 - Validation Loss: 0.8697 - Validation Accuracy: 71.77%\n",
      "Epoch 15/25 - Testing Loss: 0.8653 - Testing Accuracy: 69.00%\n",
      "Epoch 16/25 - Train Loss: 0.8030 - Train Accuracy: 73.55%\n",
      "Epoch 16/25 - Validation Loss: 0.8577 - Validation Accuracy: 73.00%\n",
      "Epoch 16/25 - Testing Loss: 0.8934 - Testing Accuracy: 70.37%\n",
      "Epoch 17/25 - Train Loss: 0.7898 - Train Accuracy: 74.12%\n",
      "Epoch 17/25 - Validation Loss: 0.8489 - Validation Accuracy: 72.02%\n",
      "Epoch 17/25 - Testing Loss: 0.8815 - Testing Accuracy: 68.91%\n",
      "Epoch 18/25 - Train Loss: 0.7856 - Train Accuracy: 74.39%\n",
      "Epoch 18/25 - Validation Loss: 0.9236 - Validation Accuracy: 70.05%\n",
      "Epoch 18/25 - Testing Loss: 0.9184 - Testing Accuracy: 69.21%\n",
      "Epoch 19/25 - Train Loss: 0.7749 - Train Accuracy: 74.80%\n",
      "Epoch 19/25 - Validation Loss: 0.7989 - Validation Accuracy: 74.06%\n",
      "Epoch 19/25 - Testing Loss: 0.8609 - Testing Accuracy: 68.09%\n",
      "Epoch 20/25 - Train Loss: 0.7641 - Train Accuracy: 75.15%\n",
      "Epoch 20/25 - Validation Loss: 0.8339 - Validation Accuracy: 74.24%\n",
      "Epoch 20/25 - Testing Loss: 0.9132 - Testing Accuracy: 69.76%\n",
      "Epoch 21/25 - Train Loss: 0.7606 - Train Accuracy: 75.52%\n",
      "Epoch 21/25 - Validation Loss: 0.8173 - Validation Accuracy: 73.77%\n",
      "Epoch 21/25 - Testing Loss: 0.8379 - Testing Accuracy: 72.53%\n",
      "Epoch 22/25 - Train Loss: 0.7559 - Train Accuracy: 75.61%\n",
      "Epoch 22/25 - Validation Loss: 0.7841 - Validation Accuracy: 75.30%\n",
      "Epoch 22/25 - Testing Loss: 0.8154 - Testing Accuracy: 71.44%\n",
      "Epoch 23/25 - Train Loss: 0.7485 - Train Accuracy: 75.84%\n",
      "Epoch 23/25 - Validation Loss: 0.7956 - Validation Accuracy: 74.24%\n",
      "Epoch 23/25 - Testing Loss: 0.7866 - Testing Accuracy: 71.78%\n",
      "Epoch 24/25 - Train Loss: 0.7374 - Train Accuracy: 76.13%\n",
      "Epoch 24/25 - Validation Loss: 0.7613 - Validation Accuracy: 76.03%\n",
      "Epoch 24/25 - Testing Loss: 0.7995 - Testing Accuracy: 72.98%\n",
      "Epoch 25/25 - Train Loss: 0.7348 - Train Accuracy: 76.45%\n",
      "Epoch 25/25 - Validation Loss: 0.7881 - Validation Accuracy: 73.99%\n",
      "Epoch 25/25 - Testing Loss: 0.8056 - Testing Accuracy: 72.36%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    cnt = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels1 = labels//10\n",
    "        labels2 = labels%10\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        image = images.clone()\n",
    "        image1 = images.clone()\n",
    "        left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "        image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "        right_half = images[:, :, :, images.shape[3] // 2 :]\n",
    "        image1[:, :, :, :(image1.shape[3] // 2)] = right_half\n",
    "        outputs = model(image)\n",
    "        outputs1 = model1(image1)\n",
    "        loss = criterion(outputs, labels1)\n",
    "        loss1 = criterion1(outputs1,labels2)\n",
    "        loss.backward()\n",
    "        loss1.backward()\n",
    "        optimizer.step()\n",
    "        optimizer1.step()\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, predicted1 = torch.max(outputs1.data, 1)\n",
    "        total_train += 2*labels.size(0)\n",
    "        for i in range(labels.size(0)):\n",
    "            if(predicted[i] == labels[i]//10):\n",
    "                correct_train += 1\n",
    "            if(predicted1[i] == labels[i]%10):\n",
    "                correct_train += 1\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {average_train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    model1.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels1 = labels // 10\n",
    "            labels2 = labels % 10\n",
    "            image = images.clone()\n",
    "            image1 = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            right_half = images[:, :, :, images.shape[3] // 2 :]\n",
    "            image1[:, :, :, :(image1.shape[3] // 2)] = right_half\n",
    "            outputs = model(image)\n",
    "            outputs1 = model1(image1)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            loss1 = criterion1(outputs1,labels2)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)\n",
    "            total_val += 2*labels.size(0)\n",
    "            for i in range(labels.size(0)):\n",
    "                if(predicted[i] == labels[i]//10):\n",
    "                    correct_val += 1\n",
    "                if(predicted1[i] == labels[i]%10):\n",
    "                    correct_val += 1\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {average_val_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    model1.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels1 = labels // 10\n",
    "            labels2 = labels % 10\n",
    "            image = images.clone()\n",
    "            image1 = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            right_half = images[:, :, :, images.shape[3] // 2 :]\n",
    "            image1[:, :, :, :(image1.shape[3] // 2)] = right_half\n",
    "            outputs = model(image)\n",
    "            outputs1 = model1(image1)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            loss1 = criterion1(outputs1,labels2)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)\n",
    "            total_val += 2*labels.size(0)\n",
    "            for i in range(labels.size(0)):\n",
    "                if(predicted[i] == labels[i]//10):\n",
    "                    correct_val += 1\n",
    "                if(predicted1[i] == labels[i]%10):\n",
    "                    correct_val += 1\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Testing Loss: {average_val_loss:.4f} - Testing Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a35d80f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.1171 - Train Accuracy: 96.44%\n",
      "Epoch 1/5 - Validation Loss: 0.0829 - Validation Accuracy: 97.42%\n",
      "Epoch 1/5 - Testing Loss: 0.0991 - Testing Accuracy: 97.00%\n",
      "Epoch 2/5 - Train Loss: 0.1029 - Train Accuracy: 96.87%\n",
      "Epoch 2/5 - Validation Loss: 0.0755 - Validation Accuracy: 97.78%\n",
      "Epoch 2/5 - Testing Loss: 0.0821 - Testing Accuracy: 97.47%\n",
      "Epoch 3/5 - Train Loss: 0.0926 - Train Accuracy: 97.21%\n",
      "Epoch 3/5 - Validation Loss: 0.0868 - Validation Accuracy: 97.53%\n",
      "Epoch 3/5 - Testing Loss: 0.0810 - Testing Accuracy: 97.75%\n",
      "Epoch 4/5 - Train Loss: 0.0836 - Train Accuracy: 97.35%\n",
      "Epoch 4/5 - Validation Loss: 0.0753 - Validation Accuracy: 97.86%\n",
      "Epoch 4/5 - Testing Loss: 0.0861 - Testing Accuracy: 97.48%\n",
      "Epoch 5/5 - Train Loss: 0.0785 - Train Accuracy: 97.52%\n",
      "Epoch 5/5 - Validation Loss: 0.0758 - Validation Accuracy: 97.85%\n",
      "Epoch 5/5 - Testing Loss: 0.0923 - Testing Accuracy: 97.39%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    cnt = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels1 = labels//10\n",
    "        # class_names = train_dataset.classes\n",
    "        optimizer.zero_grad()\n",
    "        image = images.clone()\n",
    "        left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "        # print(left_half.shape)\n",
    "        image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "        # print(images.shape)\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, labels1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        for i in range(labels.size(0)):\n",
    "            if(predicted[i] == labels1[i]):\n",
    "                correct_train += 1\n",
    "        # cnt += 1\n",
    "        # if(cnt == 10):\n",
    "        #     break\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {average_train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels1 = labels // 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {average_val_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels1 = labels // 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Testing Loss: {average_val_loss:.4f} - Testing Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "426dc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ee40524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.1110 - Train Accuracy: 96.71%\n",
      "Epoch 1/3 - Validation Loss: 0.0776 - Validation Accuracy: 97.67%\n",
      "Epoch 1/3 - Testing Loss: 0.0646 - Testing Accuracy: 97.91%\n",
      "Epoch 2/3 - Train Loss: 0.0867 - Train Accuracy: 97.27%\n",
      "Epoch 2/3 - Validation Loss: 0.0762 - Validation Accuracy: 97.79%\n",
      "Epoch 2/3 - Testing Loss: 0.0794 - Testing Accuracy: 97.61%\n",
      "Epoch 3/3 - Train Loss: 0.0784 - Train Accuracy: 97.59%\n",
      "Epoch 3/3 - Validation Loss: 0.0668 - Validation Accuracy: 98.06%\n",
      "Epoch 3/3 - Testing Loss: 0.0618 - Testing Accuracy: 97.95%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    cnt = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels1 = labels % 10\n",
    "        # class_names = train_dataset.classes\n",
    "        optimizer.zero_grad()\n",
    "        image = images.clone()\n",
    "        left_half = images[:, :, :, images.shape[3] // 2:]\n",
    "        # print(left_half.shape)\n",
    "        image[:, :, :, :(image.shape[3] // 2)] = left_half\n",
    "        # print(images.shape)\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, labels1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        for i in range(labels.size(0)):\n",
    "            if(predicted[i] == labels1[i]):\n",
    "                correct_train += 1\n",
    "        # cnt += 1\n",
    "        # if(cnt == 10):\n",
    "        #     break\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {average_train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels1 = labels % 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, images.shape[3] // 2:]\n",
    "            image[:, :, :, :(image.shape[3] // 2)] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {average_val_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels1 = labels % 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, images.shape[3] // 2:]\n",
    "            image[:, :, :, :(image.shape[3] // 2)] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Testing Loss: {average_val_loss:.4f} - Testing Accuracy: {val_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

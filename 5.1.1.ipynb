{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94047bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f3c4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, train_y = [], []\n",
    "# test_x, test_y = [], []\n",
    "# valid_x, valid_y = [], []\n",
    "\n",
    "# def load_images_and_labels(root_folder):\n",
    "#     for folder_name in os.listdir(root_folder):\n",
    "#         if not os.path.isdir(os.path.join(root_folder, folder_name)):\n",
    "#             continue\n",
    "\n",
    "#         # Convert folder name (label) to a list of integers\n",
    "#         label = [int(digit) for digit in folder_name]\n",
    "#         if(label[0]==label[1]):\n",
    "#             continue\n",
    "\n",
    "#         for image_name in os.listdir(os.path.join(root_folder, folder_name)):\n",
    "#             if image_name.endswith(\".png\"):  # Assuming images are in PNG format\n",
    "#                 image_path = os.path.join(root_folder, folder_name, image_name)\n",
    "\n",
    "#                 # Open the image and convert it to grayscale\n",
    "#                 image = Image.open(image_path).convert('L')\n",
    "\n",
    "#                 # Convert the image to a numpy array\n",
    "#                 image_array = np.array(image)\n",
    "\n",
    "#                 # Normalize pixel values (optional)\n",
    "#                 image_array = image_array / 255.0\n",
    "\n",
    "#                 if \"train\" in root_folder:\n",
    "#                     train_x.append(image)\n",
    "#                     train_y.append(label)\n",
    "#                 elif \"test\" in root_folder:\n",
    "#                     test_x.append(image)\n",
    "#                     test_y.append(label)\n",
    "#                 elif \"val\" in root_folder:\n",
    "#                     valid_x.append(image)\n",
    "#                     valid_y.append(label)\n",
    "\n",
    "\n",
    "# load_images_and_labels(\"double_mnist/train\")\n",
    "# load_images_and_labels(\"double_mnist/val\")\n",
    "# load_images_and_labels(\"double_mnist/test\")\n",
    "# # print(train_x[0])\n",
    "# # for i in range(len(train_x)):\n",
    "# #     print(train_x[i], train_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2647793b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor(): argument 'dtype' must be torch.dtype, not module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 3/5.1v4.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%203/5.1v4.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Convert train_x, train_y, valid_x, valid_y, test_x, and test_y to PyTorch tensors\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%203/5.1v4.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print(train_x)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%203/5.1v4.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(train_x, dtype\u001b[39m=\u001b[39;49mImage)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%203/5.1v4.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_y, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%203/5.1v4.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m valid_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(valid_x, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor(): argument 'dtype' must be torch.dtype, not module"
     ]
    }
   ],
   "source": [
    "# # Convert train_x, train_y, valid_x, valid_y, test_x, and test_y to PyTorch tensors\n",
    "# # print(train_x)\n",
    "# train_x = torch.tensor(train_x, dtype=)\n",
    "# train_y = torch.tensor(train_y, dtype=torch.int64)\n",
    "# valid_x = torch.tensor(valid_x, dtype=torch.float32)\n",
    "# valid_y = torch.tensor(valid_y, dtype=torch.int64)\n",
    "# test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "# test_y = torch.tensor(test_y, dtype=torch.int64)\n",
    "\n",
    "# # Create TensorDatasets for train, validation, and test\n",
    "# train_dataset = TensorDataset(torch.tensor(train_x), torch.tensor(train_y))\n",
    "# valid_dataset = TensorDataset(torch.tensor(valid_x), torch.tensor(valid_y))\n",
    "# test_dataset = TensorDataset(torch.tensor(test_x), torch.tensor(test_y))\n",
    "# # Set batch size for each DataLoader\n",
    "# batch_size = 32  # You can adjust this based on your hardware and needs\n",
    "\n",
    "# # Create DataLoader instances for train, validation, and test\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "397f5c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8, 5],\n",
      "        [1, 4],\n",
      "        [4, 2],\n",
      "        [2, 6],\n",
      "        [5, 9],\n",
      "        [5, 3],\n",
      "        [8, 1],\n",
      "        [8, 2],\n",
      "        [7, 4],\n",
      "        [7, 2],\n",
      "        [2, 8],\n",
      "        [5, 4],\n",
      "        [6, 5],\n",
      "        [5, 0],\n",
      "        [1, 2],\n",
      "        [7, 6],\n",
      "        [2, 0],\n",
      "        [4, 5],\n",
      "        [4, 1],\n",
      "        [4, 3],\n",
      "        [7, 5],\n",
      "        [7, 0],\n",
      "        [5, 3],\n",
      "        [7, 6],\n",
      "        [0, 1],\n",
      "        [1, 2],\n",
      "        [1, 8],\n",
      "        [8, 1],\n",
      "        [2, 0],\n",
      "        [5, 0],\n",
      "        [8, 7],\n",
      "        [8, 9]])\n",
      "32\n",
      "torch.Size([32, 64, 64])\n",
      "torch.Size([32, 2])\n",
      "tensor([4, 9])\n"
     ]
    }
   ],
   "source": [
    "# # print(train_loader.shape)\n",
    "# print(labels)\n",
    "# print(batch_size)\n",
    "# for images, labels in test_loader:\n",
    "#     print(images.shape)\n",
    "#     print(labels.shape)\n",
    "#     print(labels[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cdeb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '04', '05', '06', '08', '09', '11', '12', '13', '14', '15', '16', '18', '19', '20', '21', '23', '24', '26', '28', '29', '30', '31', '33', '35', '37', '38', '41', '42', '43', '44', '45', '50', '51', '53', '54', '56', '59', '60', '62', '63', '65', '69', '70', '72', '74', '75', '76', '77', '79', '81', '82', '84', '85', '87', '88', '89', '90', '91', '94', '95', '97', '98']\n",
      "Dataset CustomImageFolder\n",
      "    Number of datapoints: 64000\n",
      "    Root location: double_mnist/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=1)\n",
      "               Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "['03', '07', '10', '22', '27', '34', '39', '40', '48', '52', '58', '61', '64', '71', '93', '99']\n",
      "tensor([50,  6, 50, 81, 95, 14, 43,  6, 16, 50, 74, 56, 95,  1, 70, 53, 53, 69,\n",
      "        14, 33, 33, 77, 87, 26, 60, 98, 94, 26, 77, 24, 44, 90, 74,  9, 42, 79,\n",
      "        60, 37, 51, 60, 79, 51, 81, 82, 41, 44, 79,  6, 89, 41, 33, 23, 51, 42,\n",
      "        35, 69, 37, 87, 84, 62, 35, 19, 69, 85])\n",
      "tensor(3)\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if not already\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data_path = 'double_mnist/train'\n",
    "val_data_path = 'double_mnist/val'\n",
    "test_data_path = 'double_mnist/test'\n",
    "class CustomImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(CustomImageFolder, self).__init__(root, transform=transform)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # Assign labels based on folder name\n",
    "        folder_name = os.path.basename(os.path.dirname(path))\n",
    "        return sample, int(folder_name)  # Convert the folder name to an integer label\n",
    "# Create ImageFolder datasets for training, validation, and test sets\n",
    "train_dataset = CustomImageFolder(train_data_path, transform=transform)\n",
    "val_dataset = CustomImageFolder(val_data_path, transform=transform)\n",
    "test_dataset = CustomImageFolder(test_data_path, transform=transform)\n",
    "print(train_dataset.classes)\n",
    "class_to_label = {str(i).zfill(2): i for i in range(100)}\n",
    "# Define batch sizes\n",
    "batch_size = 64\n",
    "print(train_dataset)\n",
    "# Create DataLoader instances for training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 10\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "class_names = val_dataset.classes\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    break\n",
    "for images, labels in val_loader:\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    print(labels[0])\n",
    "    print(labels)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2471806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, kernel_size=3, pool_size=2, stride=2, dropout_rate=0.5):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=kernel_size, padding=int((kernel_size - 1) / 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=pool_size, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=int((kernel_size - 1) / 2))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=pool_size, stride=stride)\n",
    "        var1 = int((28 - pool_size) / stride) + 1\n",
    "        self.dim = int((var1 - pool_size) / stride) + 1\n",
    "        self.fc1 = nn.Linear(64 * self.dim * self.dim, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * self.dim * self.dim)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN(input_channels=1, num_classes=num_classes)\n",
    "model1 = SimpleCNN(input_channels=1, num_classes=num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3909ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.1039 - Train Accuracy: 96.81%\n",
      "Epoch 1/5 - Validation Loss: 0.0837 - Validation Accuracy: 97.25%\n",
      "Epoch 1/5 - Testing Loss: 0.0879 - Testing Accuracy: 97.08%\n",
      "Epoch 2/5 - Train Loss: 0.0867 - Train Accuracy: 98.02%\n",
      "Epoch 2/5 - Validation Loss: 0.0832 - Validation Accuracy: 97.44%\n",
      "Epoch 2/5 - Testing Loss: 0.0866 - Testing Accuracy: 97.42%\n",
      "Epoch 3/5 - Train Loss: 0.0796 - Train Accuracy: 98.25%\n",
      "Epoch 3/5 - Validation Loss: 0.0773 - Validation Accuracy: 97.48%\n",
      "Epoch 3/5 - Testing Loss: 0.0819 - Testing Accuracy: 97.59%\n",
      "Epoch 4/5 - Train Loss: 0.0728 - Train Accuracy: 98.42%\n",
      "Epoch 4/5 - Validation Loss: 0.0863 - Validation Accuracy: 97.60%\n",
      "Epoch 4/5 - Testing Loss: 0.0932 - Testing Accuracy: 97.58%\n",
      "Epoch 5/5 - Train Loss: 0.0635 - Train Accuracy: 98.62%\n",
      "Epoch 5/5 - Validation Loss: 0.0839 - Validation Accuracy: 97.74%\n",
      "Epoch 5/5 - Testing Loss: 0.0784 - Testing Accuracy: 97.82%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    cnt = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels1 = labels//10\n",
    "        labels2 = labels%10\n",
    "        # class_names = train_dataset.classes\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        image = images.clone()\n",
    "        image1 = images.clone()\n",
    "        left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "        # print(left_half.shape)\n",
    "        image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "        right_half = images[:, :, :, images.shape[3] // 2 :]\n",
    "        image1[:, :, :, :(image1.shape[3] // 2)] = right_half\n",
    "        # print(images.shape)\n",
    "        outputs = model(image)\n",
    "        outputs1 = model1(image1)\n",
    "        loss = criterion(outputs, labels1)\n",
    "        loss1 = criterion1(outputs1,labels2)\n",
    "        loss.backward()\n",
    "        loss1.backward()\n",
    "        optimizer.step()\n",
    "        optimizer1.step()\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, predicted1 = torch.max(outputs1.data, 1)\n",
    "        total_train += 2*labels.size(0)\n",
    "        for i in range(labels.size(0)):\n",
    "            # cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted[i])\n",
    "            #     print(predicted1[i])\n",
    "            #     print(labels[i])\n",
    "            #     cnt=0\n",
    "            #     break\n",
    "            if(predicted[i] == labels[i]//10):\n",
    "                correct_train += 1\n",
    "            if(predicted1[i] == labels[i]%10):\n",
    "                correct_train += 1\n",
    "        # cnt += 1\n",
    "        # if(cnt == 10):\n",
    "        #     break\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {average_train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    model1.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels1 = labels // 10\n",
    "            labels2 = labels % 10\n",
    "            image = images.clone()\n",
    "            image1 = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            right_half = images[:, :, :, images.shape[3] // 2 :]\n",
    "            image1[:, :, :, :(image1.shape[3] // 2)] = right_half\n",
    "            outputs = model(image)\n",
    "            outputs1 = model1(image1)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            loss1 = criterion1(outputs1,labels2)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)\n",
    "            total_val += 2*labels.size(0)\n",
    "            for i in range(labels.size(0)):\n",
    "                if(predicted[i] == labels[i]//10):\n",
    "                    correct_val += 1\n",
    "                if(predicted1[i] == labels[i]%10):\n",
    "                    correct_val += 1\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {average_val_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    model1.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels1 = labels // 10\n",
    "            labels2 = labels % 10\n",
    "            image = images.clone()\n",
    "            image1 = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            right_half = images[:, :, :, images.shape[3] // 2 :]\n",
    "            image1[:, :, :, :(image1.shape[3] // 2)] = right_half\n",
    "            outputs = model(image)\n",
    "            outputs1 = model1(image1)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            loss1 = criterion1(outputs1,labels2)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)\n",
    "            total_val += 2*labels.size(0)\n",
    "            for i in range(labels.size(0)):\n",
    "                if(predicted[i] == labels[i]//10):\n",
    "                    correct_val += 1\n",
    "                if(predicted1[i] == labels[i]%10):\n",
    "                    correct_val += 1\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Testing Loss: {average_val_loss:.4f} - Testing Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a35d80f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.1171 - Train Accuracy: 96.44%\n",
      "Epoch 1/5 - Validation Loss: 0.0829 - Validation Accuracy: 97.42%\n",
      "Epoch 1/5 - Testing Loss: 0.0991 - Testing Accuracy: 97.00%\n",
      "Epoch 2/5 - Train Loss: 0.1029 - Train Accuracy: 96.87%\n",
      "Epoch 2/5 - Validation Loss: 0.0755 - Validation Accuracy: 97.78%\n",
      "Epoch 2/5 - Testing Loss: 0.0821 - Testing Accuracy: 97.47%\n",
      "Epoch 3/5 - Train Loss: 0.0926 - Train Accuracy: 97.21%\n",
      "Epoch 3/5 - Validation Loss: 0.0868 - Validation Accuracy: 97.53%\n",
      "Epoch 3/5 - Testing Loss: 0.0810 - Testing Accuracy: 97.75%\n",
      "Epoch 4/5 - Train Loss: 0.0836 - Train Accuracy: 97.35%\n",
      "Epoch 4/5 - Validation Loss: 0.0753 - Validation Accuracy: 97.86%\n",
      "Epoch 4/5 - Testing Loss: 0.0861 - Testing Accuracy: 97.48%\n",
      "Epoch 5/5 - Train Loss: 0.0785 - Train Accuracy: 97.52%\n",
      "Epoch 5/5 - Validation Loss: 0.0758 - Validation Accuracy: 97.85%\n",
      "Epoch 5/5 - Testing Loss: 0.0923 - Testing Accuracy: 97.39%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    cnt = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels1 = labels//10\n",
    "        # class_names = train_dataset.classes\n",
    "        optimizer.zero_grad()\n",
    "        image = images.clone()\n",
    "        left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "        # print(left_half.shape)\n",
    "        image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "        # print(images.shape)\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, labels1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        for i in range(labels.size(0)):\n",
    "            if(predicted[i] == labels1[i]):\n",
    "                correct_train += 1\n",
    "        # cnt += 1\n",
    "        # if(cnt == 10):\n",
    "        #     break\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {average_train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels1 = labels // 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {average_val_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels1 = labels // 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Testing Loss: {average_val_loss:.4f} - Testing Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "426dc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ee40524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.1110 - Train Accuracy: 96.71%\n",
      "Epoch 1/3 - Validation Loss: 0.0776 - Validation Accuracy: 97.67%\n",
      "Epoch 1/3 - Testing Loss: 0.0646 - Testing Accuracy: 97.91%\n",
      "Epoch 2/3 - Train Loss: 0.0867 - Train Accuracy: 97.27%\n",
      "Epoch 2/3 - Validation Loss: 0.0762 - Validation Accuracy: 97.79%\n",
      "Epoch 2/3 - Testing Loss: 0.0794 - Testing Accuracy: 97.61%\n",
      "Epoch 3/3 - Train Loss: 0.0784 - Train Accuracy: 97.59%\n",
      "Epoch 3/3 - Validation Loss: 0.0668 - Validation Accuracy: 98.06%\n",
      "Epoch 3/3 - Testing Loss: 0.0618 - Testing Accuracy: 97.95%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    cnt = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels1 = labels % 10\n",
    "        # class_names = train_dataset.classes\n",
    "        optimizer.zero_grad()\n",
    "        image = images.clone()\n",
    "        left_half = images[:, :, :, images.shape[3] // 2:]\n",
    "        # print(left_half.shape)\n",
    "        image[:, :, :, :(image.shape[3] // 2)] = left_half\n",
    "        # print(images.shape)\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, labels1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        for i in range(labels.size(0)):\n",
    "            if(predicted[i] == labels1[i]):\n",
    "                correct_train += 1\n",
    "        # cnt += 1\n",
    "        # if(cnt == 10):\n",
    "        #     break\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {average_train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels1 = labels % 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, images.shape[3] // 2:]\n",
    "            image[:, :, :, :(image.shape[3] // 2)] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Validation Loss: {average_val_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels1 = labels % 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, images.shape[3] // 2:]\n",
    "            image[:, :, :, :(image.shape[3] // 2)] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels1)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels1).sum().item()\n",
    "            cnt += 1\n",
    "            # if(cnt == 10):\n",
    "            #     print(predicted)\n",
    "            #     print(labels1)\n",
    "            #     print(labels)\n",
    "            #     break\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    average_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Testing Loss: {average_val_loss:.4f} - Testing Accuracy: {val_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
